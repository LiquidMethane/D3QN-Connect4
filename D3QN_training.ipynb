{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fae50-2bad-4c14-a97a-a7a08b003f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from D3QNAgent import Agent\n",
    "from kaggle_environments import evaluate, make, utils\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.render()\n",
    "\n",
    "nstep_lookahead_agent = './fast_Nstep_lookahead_agent.py'\n",
    "random_agent = 'random'\n",
    "negamax_agent = 'negamax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cab155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot(rewards, winloss):\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, dpi=120, figsize=(12.8, 3.6))\n",
    "    ax1.plot(np.array(rewards), color='orange', linestyle='-')\n",
    "    # ax1.legend(loc='lower right')\n",
    "    ax1.set_ylabel('reward')\n",
    "    ax1.set_xlabel('episode')\n",
    "    ax1.title.set_text('Agent Total Rewards with Moving Average 100 Episodes')\n",
    "    \n",
    "\n",
    "    # ax2.plot(np.array(winloss), color='royalblue', linestyle='-')\n",
    "    # # ax2.legend(loc='lower right')\n",
    "    # ax2.set_ylabel('Expectation')\n",
    "    # ax2.set_xlabel('episode')\n",
    "    # ax2.title.set_text('Win/Loss/Draw Expectation with Moving Average 100 Episodes')\n",
    "    plt.show()\n",
    "\n",
    "    print('episode', i, \n",
    "        # '\\ttotal rewards %.1f' % tot_reward, \n",
    "        '\\taverage rewards %.3f' % avg_rewards[i], \n",
    "        # '\\tresult ', reward, \n",
    "        # '\\twin/loss ratio %.3f' % wl_ratio[i], \n",
    "        '\\tepsilon %.3f' % agent.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9886f-f084-40ff-aaf6-2a8e42471dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env.configuration, \n",
    "    lr=1e-4, \n",
    "    gamma=0.95, \n",
    "    batch_size=64,    #### REMEMBER TO CHANGE IT BACK ####\n",
    "    epsilon=1,        #### REMEMBER TO CHANGE IT BACK ####\n",
    "    eps_dec=0.9995, \n",
    "    eps_min=0.1, \n",
    "    buff_size=100_000, \n",
    "    d1_dims=128, \n",
    "    d2_dims=128,\n",
    "    replace_target_weight=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards, avg_rewards, result, eps_history, wl_ratio = [], [], [], [], []\n",
    "\n",
    "# None means the agent being trained\n",
    "trainer = env.train([negamax_agent, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b77096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_episodes = 30000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81630100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "\n",
    "    observation = trainer.reset()\n",
    "    board = observation['board']\n",
    "    marker = observation.mark\n",
    "    obs = np.array(board, dtype=np.float32).reshape(6, 7, 1)\n",
    "\n",
    "    if marker == 1:\n",
    "        obs[obs == 2] = -10\n",
    "        obs[obs == 1] = 10\n",
    "    else:\n",
    "        obs[obs == 1] = -10\n",
    "        obs[obs == 2] = 10\n",
    "    \n",
    "    # print(obs.reshape(6, 7))\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # choose the best action\n",
    "        action = agent.choose_action(obs)\n",
    "        print(\"action: \", action)\n",
    "        \n",
    "        # step the environment with action\n",
    "        # store all returns\n",
    "        observation_, reward, done, info = trainer.step(action)\n",
    "        board_ = observation_['board']\n",
    "        marker_ = observation_.mark\n",
    "        obs_ = np.array(board_, dtype=np.float32).reshape(6, 7, 1)\n",
    "        if marker_ == 1:\n",
    "            obs_[obs_ == 2] = -10\n",
    "            obs_[obs_ == 1] = 10\n",
    "        else:\n",
    "            obs_[obs_ == 1] = -10\n",
    "            obs_[obs_ == 2] = 10\n",
    "\n",
    "        # print(obs_.reshape(6, 7))\n",
    "\n",
    "        # calculate agent reward from environment response\n",
    "        agent_reward = agent.get_agent_reward(reward, done)\n",
    "        \n",
    "        tot_reward += agent_reward\n",
    "        \n",
    "        # store this transition\n",
    "        agent.update_replay_buffer((obs, action, reward, obs_, done))\n",
    "        \n",
    "        # update the current obs with new obs\n",
    "        obs = obs_\n",
    "        \n",
    "        \n",
    "        agent.learn()\n",
    "    \n",
    "    agent.evolve()\n",
    "\n",
    "    # store final game result\n",
    "    result.append(reward)\n",
    "    # store epsilon\n",
    "    eps_history.append(agent.epsilon)\n",
    "    # store total rewards\n",
    "    rewards.append(tot_reward)\n",
    "    # store average rewards\n",
    "    avg_rewards.append(np.mean(rewards[-100:]))\n",
    "    wl_ratio.append(np.mean(result[-100:]))\n",
    "    \n",
    "\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    #     live_plot(avg_rewards, wl_ratio)\n",
    "        \n",
    "    env.render()\n",
    "    \n",
    "# live_plot(avg_rewards, wl_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af99193-7cb5-4411-89e0-e7ebedc9900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model('./models/negamax_30000_conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa18ee-07d2-46bb-87b9-7eee66f86a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from D3QNAgent import Agent\n",
    "def my_agent(obs, config):\n",
    "    # define agent\n",
    "    agent = Agent(config)\n",
    "    agent.load_model('./models/negamax_30000_conv')\n",
    "\n",
    "    # preprocessing of the state\n",
    "    board = obs['board']\n",
    "    marker = obs.mark\n",
    "    state = np.array(board, dtype=np.float32).reshape(6, 7, 1)\n",
    "\n",
    "    if marker == 1:\n",
    "        obs[obs == 2] = -10\n",
    "        obs[obs == 1] = 10\n",
    "    else:\n",
    "        obs[obs == 1] = -10\n",
    "        obs[obs == 2] = 10\n",
    "\n",
    "    return agent.choose_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606ec18-20a8-4713-88a5-12268a735182",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# Play as the first agent against default \"random\" agent.\n",
    "env.run([my_agent, nstep_lookahead_agent])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fc3ef-488c-424e-a332-35f2640006f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_reward(rewards):\n",
    "#     return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "# print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6a6fa",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "PyCharm (Project)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

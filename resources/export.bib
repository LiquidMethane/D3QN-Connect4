@article{,
   abstract = {The popular Q-learning algorithm is known to overestimate action values under
certain conditions. It was not previously known whether, in practice, such
overestimations are common, whether they harm performance, and whether they can
generally be prevented. In this paper, we answer all these questions
affirmatively. In particular, we first show that the recent DQN algorithm,
which combines Q-learning with a deep neural network, suffers from substantial
overestimations in some games in the Atari 2600 domain. We then show that the
idea behind the Double Q-learning algorithm, which was introduced in a tabular
setting, can be generalized to work with large-scale function approximation. We
propose a specific adaptation to the DQN algorithm and show that the resulting
algorithm not only reduces the observed overestimations, as hypothesized, but
that this also leads to much better performance on several games.},
   author = {Hado Van Hasselt and Arthur Guez and David Silver},
   doi = {10.48550/arxiv.1509.06461},
   isbn = {9781577357605},
   journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
   month = {9},
   pages = {2094-2100},
   publisher = {AAAI press},
   title = {Deep Reinforcement Learning with Double Q-learning},
   url = {https://arxiv.org/abs/1509.06461v3},
   year = {2015},
}
@article{Mnih2013,
   abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   doi = {10.48550/arxiv.1312.5602},
   month = {12},
   title = {Playing Atari with Deep Reinforcement Learning},
   url = {https://arxiv.org/abs/1312.5602v1},
   year = {2013},
}
@article{Wang2015,
   abstract = {In recent years there have been many successes of using deep representations
in reinforcement learning. Still, many of these applications use conventional
architectures, such as convolutional networks, LSTMs, or auto-encoders. In this
paper, we present a new neural network architecture for model-free
reinforcement learning. Our dueling network represents two separate estimators:
one for the state value function and one for the state-dependent action
advantage function. The main benefit of this factoring is to generalize
learning across actions without imposing any change to the underlying
reinforcement learning algorithm. Our results show that this architecture leads
to better policy evaluation in the presence of many similar-valued actions.
Moreover, the dueling architecture enables our RL agent to outperform the
state-of-the-art on the Atari 2600 domain.},
   author = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado Van Hasselt and Marc Lanctot and Nando De Frcitas},
   doi = {10.48550/arxiv.1511.06581},
   isbn = {9781510829008},
   journal = {33rd International Conference on Machine Learning, ICML 2016},
   month = {11},
   pages = {2939-2947},
   publisher = {International Machine Learning Society (IMLS)},
   title = {Dueling Network Architectures for Deep Reinforcement Learning},
   volume = {4},
   url = {https://arxiv.org/abs/1511.06581v3},
   year = {2015},
}
